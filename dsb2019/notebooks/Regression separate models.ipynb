{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from target_encoding import TargetEncoderClassifier, TargetEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import reduce\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json\n",
    "from functools import partial\n",
    "from dsb2019.models.coeff import ThresholdClassifier\n",
    "\n",
    "from dsb2019.models.tracking import track_experiment, track_submission_info\n",
    "from dsb2019.data.validation import InstallationFold, cross_validate, quad_kappa\n",
    "from dsb2019.visualization import session_browser\n",
    "from dsb2019.data import DATA_DIR\n",
    "from dsb2019.data import adv_validation\n",
    "from dsb2019.models import MODELS_DIR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, Trials, tpe, STATUS_OK\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_rows=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR / 'raw/train.csv')\n",
    "test = pd.read_csv(DATA_DIR / 'raw/test.csv')\n",
    "train_labels = pd.read_csv(DATA_DIR / 'raw/train_labels.csv')\n",
    "submission = pd.read_csv(DATA_DIR / 'raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           27253bdc\n",
       "1           27253bdc\n",
       "2           77261ab5\n",
       "3           b2dba42b\n",
       "4           1bb5fbdb\n",
       "              ...   \n",
       "11341037    ab3136ba\n",
       "11341038    27253bdc\n",
       "11341039    27253bdc\n",
       "11341040    27253bdc\n",
       "11341041    27253bdc\n",
       "Name: event_id, Length: 11341042, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = ['Scrub-A-Dub', 'All Star Sorting',\n",
    "       'Air Show', 'Crystals Rule', \n",
    "       'Dino Drink', 'Bubble Bath', 'Dino Dive', 'Chow Time',\n",
    "       'Pan Balance', 'Happy Camel',\n",
    "       'Leaf Leader']\n",
    "assessments = ['Mushroom Sorter (Assessment)',\n",
    "       'Bird Measurer (Assessment)',\n",
    "       'Cauldron Filler (Assessment)',\n",
    "       'Cart Balancer (Assessment)', 'Chest Sorter (Assessment)']\n",
    "worlds = ['NONE', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES']\n",
    "\n",
    "def unwrap_event_data(df):\n",
    "    unwrapped=pd.DataFrame(data=list(df.event_data.apply(json.loads).values))\n",
    "    return pd.concat([unwrapped.reset_index(),df.reset_index()],axis=1)\n",
    "\n",
    "\n",
    "def process_installations(train_labels, train, process_log):\n",
    "    result = []\n",
    "    train=train.sort_values(\"timestamp\")\n",
    "    installations = train.groupby(\"installation_id\")\n",
    "    for i, game_session, title, installation_id, accuracy_group in tqdm(train_labels[[\"game_session\", \"title\", \"installation_id\", \"accuracy_group\"]].itertuples(), \n",
    "                                                              total=len(train_labels), position=0):\n",
    "        player_log = installations.get_group(installation_id).reset_index()\n",
    "        log_length = player_log[(player_log.game_session==game_session) & (player_log.title==title)].index[0]\n",
    "        player_log = player_log.iloc[:(log_length + 1)]\n",
    "        player_log[\"accuracy_group\"] = accuracy_group\n",
    "        player_log[\"target_game_session\"] = game_session\n",
    "        features = process_log(player_log)\n",
    "        features[\"installation_id\"] = installation_id\n",
    "        features[\"accuracy_group\"] = accuracy_group\n",
    "        result.append(features)\n",
    "    return pd.DataFrame(data=result).fillna(-1)\n",
    "\n",
    "\n",
    "def make_counters(df, column):\n",
    "    return df.groupby(column)[column].count().to_dict()\n",
    "\n",
    "    \n",
    "def process_log(df):\n",
    "    assessment_title=df.title.iloc[-1]   \n",
    "    world=df.world.iloc[-1]\n",
    "\n",
    "    history = df.iloc[:-1]\n",
    "    history = history[history.type.isin([\"Game\", \"Assessment\"])].copy()\n",
    "\n",
    "    def calculate_ratios(df):\n",
    "        n_correct=df.correct_move.sum()\n",
    "        n_incorrect=df.wrong_move.sum()\n",
    "        ratio=n_correct/(n_correct+n_incorrect)\n",
    "        return n_correct, n_incorrect, ratio\n",
    "    \n",
    "    def make_move_stats(df, title,n_lags=2):\n",
    "        df=df.copy()\n",
    "        if len(df):\n",
    "            df = unwrap_event_data(df)\n",
    "        if \"correct\" in df.columns:\n",
    "            df[\"correct_move\"] = df.correct == True\n",
    "            df[\"wrong_move\"] = df.correct == False\n",
    "        else:\n",
    "            df[\"correct_move\"]=False\n",
    "            df[\"wrong_move\"]=False\n",
    "        result = []\n",
    "        result.extend(zip([f\"n_correct_{title}\", f\"n_incorrect_{title}\", f\"global_ratio_{title}\"], calculate_ratios(df)))\n",
    "\n",
    "        if n_lags:\n",
    "            last_sessions = df.game_session.unique()[-n_lags:]\n",
    "            for i in range(n_lags):\n",
    "                if i < len(last_sessions): \n",
    "                    result.extend(zip([f\"n_correct_{title}_{i}\", f\"n_incorrect_{title} {i}\",f\"ratio_{title}_{i}\"], \n",
    "                                      calculate_ratios(df[df.game_session==last_sessions[i]])))\n",
    "                else:\n",
    "                    result.extend(zip([f\"n_correct_{title}_{i}\", f\"n_incorrect_{title}_{i}\",f\"ratio_{title}_{i}\"], [None, None, None]))\n",
    "        return {k: v for k, v in result}\n",
    "    \n",
    "    \n",
    "    result = {\"title\": assessments.index(assessment_title),\n",
    "              \"world\": worlds.index(world),\n",
    "              \"n_activities\": df[df.type==\"Activity\"].game_session.nunique(),\n",
    "              \"n_games\": df[df.type==\"Game\"].game_session.nunique(),\n",
    "              \"event_code_count\": df.event_code.nunique(),\n",
    "              \"event_id_count\": df.event_id.nunique(),\n",
    "              \"title_count\": df.title.nunique(),\n",
    "              \"n_actions\": len(df),\n",
    "              \"world_title_count\": df[df.world==world].title.nunique(),\n",
    "             }\n",
    "    for game in games:\n",
    "        stats=history[history.title==game]\n",
    "        stats_features=make_move_stats(stats, game)\n",
    "        stats_features[f\"{game}_event_code_count\"] = stats.event_code.nunique()\n",
    "        #stats_features[f\"{game}_event_id_count\"] = stats.event_id.nunique()\n",
    "        stats_features[f\"{game}_session_id_count\"] = stats.game_session.nunique()\n",
    "        stats_features[f\"{game}_n_actions\"] = len(stats)\n",
    "        result.update(stats_features)\n",
    "        result.update({f\"{game}_{k}\": v for k, v in make_counters(stats, \"event_id\").items()})\n",
    "        result.update({f\"{game}_{k}\": v for k, v in make_counters(stats, \"event_code\").items()})\n",
    "    world_games = history[history.world==world]\n",
    "    for game in games:\n",
    "        stats=world_games[world_games.title==game]\n",
    "        stats_features=make_move_stats(stats, game)\n",
    "        stats_features = {f\"world_{k}\": v for k, v in stats_features.items()}\n",
    "        stats_features[f\"world_{game}_event_code_count\"] = stats.event_code.nunique()\n",
    "        stats_features[f\"world_{game}_event_id_count\"] = stats.event_id.nunique()\n",
    "        stats_features[f\"world_{game}_session_id_count\"] = stats.game_session.nunique()\n",
    "        stats_features[f\"world_{game}_n_actions\"] = len(stats)\n",
    "        result.update(stats_features)\n",
    "        result.update({f\"world_{game}_{k}\": v for k, v in make_counters(stats, \"event_id\").items()})\n",
    "        result.update({f\"world_{game}_{k}\": v for k, v in make_counters(stats, \"event_code\").items()})\n",
    "    result.update(make_counters(history, \"event_id\"))\n",
    "    result.update(make_counters(history, \"event_code\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lgb_splits_files(assessments):\n",
    "    def binarize(arr):\n",
    "        if len(arr)==1:\n",
    "            return None\n",
    "        else:\n",
    "            split_at = len(arr)//2\n",
    "            threshold = arr[split_at]\n",
    "            left = binarize(arr[:split_at])\n",
    "            right = binarize(arr[split_at:])\n",
    "            result = {\n",
    "                \"feature\": \"title\",\n",
    "                \"threshold\": threshold,\n",
    "            }\n",
    "            if left is not None:\n",
    "                result[\"left\"]=left\n",
    "            if right is not None:\n",
    "                result[\"right\"]=right\n",
    "            return result\n",
    "    bin_ups = [x - 0.1 for x in range(len(assessments))]\n",
    "    forced_splits = binarize(bin_ups)\n",
    "    \n",
    "    def collect_bins(node, result):\n",
    "        result.add(node[\"threshold\"])\n",
    "        if \"left\" in node:\n",
    "            collect_bins(node[\"left\"], result)\n",
    "        if \"right\" in node:\n",
    "            collect_bins(node[\"right\"], result)\n",
    "    \n",
    "#     bin_upper_bound = set([])\n",
    "#     collect_bins(forced_splits, bin_upper_bound)\n",
    "#     forced_bins = [\n",
    "#         {\n",
    "#             \"feature\": \"title\",\n",
    "#             \"bin_upper_bound\": sorted(bin_upper_bound)\n",
    "#         }\n",
    "#     ]\n",
    "    return forced_splits\n",
    "forced_splits = create_lgb_splits_files(assessments)\n",
    "with open(\"forced_splits.json\", \"w\") as f:\n",
    "    json.dump(forced_splits, f)\n",
    "# with open(\"forced_bins.json\", \"w\") as f:\n",
    "#     json.dump(forced_bins, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': 'title',\n",
       " 'threshold': 1.9,\n",
       " 'left': {'feature': 'title', 'threshold': 0.9},\n",
       " 'right': {'feature': 'title',\n",
       "  'threshold': 2.9,\n",
       "  'right': {'feature': 'title', 'threshold': 3.9}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forced_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17690 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in long_scalars\n",
      " 73%|███████▎  | 12993/17690 [43:33<11:14,  6.96it/s] "
     ]
    }
   ],
   "source": [
    "train_features = process_installations(train_labels, train, process_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_features = [\"session_id_count\", \"event_id_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicate_features(features, bad_features):\n",
    "    to_remove = set([])\n",
    "    counter = 0\n",
    "    feature_names=[f for f in features.columns if f not in (\"installation_id\", \"game_session\", \"accuracy_group\")]\n",
    "    for feat_a in tqdm(feature_names):\n",
    "        for feat_b in feature_names:\n",
    "            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "                c = np.corrcoef(features[feat_a], features[feat_b])[0][1]\n",
    "                if c > 0.995:\n",
    "                    counter += 1\n",
    "                    to_remove.add(feat_b)\n",
    "                    if feat_b in bad_features or feat_a in bad_features:\n",
    "                        to_remove.add(feat_a)\n",
    "                    #print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n",
    "    for bf in bad_features:\n",
    "        to_remove.add(bf)\n",
    "    print(f\"{len(to_remove)} features were removed ({round(len(to_remove)/len(feature_names)*100, 2)}% of all features)\")\n",
    "    return list(to_remove)\n",
    "    \n",
    "duplicate_features = get_duplicate_features(train_features, bad_features)\n",
    "\n",
    "useful_features = [f for f in train_features.columns if f not in duplicate_features]\n",
    "useful_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=train_features[useful_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_quad_kappa(preds, true):\n",
    "    true = true.get_label()\n",
    "    #preds = preds.reshape((4, -1)).argmax(axis=0)\n",
    "    preds = np.rint(preds)\n",
    "    preds = np.maximum(0, preds)\n",
    "    preds = np.minimum(3, preds)\n",
    "    return \"quad_kappa\", quad_kappa(true, preds), True\n",
    "    \n",
    "    \n",
    "def train_baseline(x_train,y_train, params=None):\n",
    "    x_train_all, x_val_all,y_train_all,y_val_all = train_test_split(\n",
    "        x_train,y_train,\n",
    "        test_size=0.15,\n",
    "        random_state=2019,\n",
    "    )\n",
    "    train_set = lgb.Dataset(x_train_all, y_train_all)\n",
    "    val_set = lgb.Dataset(x_val_all, y_val_all)\n",
    "\n",
    "#     params = {\n",
    "#         'learning_rate': 0.01,\n",
    "#         'bagging_fraction': 0.9,\n",
    "#         'feature_fraction': 0.9,\n",
    "#         'num_leaves': 14,\n",
    "#         'lambda_l1': 0.1,\n",
    "#         'lambda_l2': 1,\n",
    "#         'metric': 'multiclass',\n",
    "#         'objective': 'multiclass',\n",
    "#         'num_classes': 4,\n",
    "#         'random_state': 2019\n",
    "#     }\n",
    "\n",
    "    return lgb.train(params, train_set, num_boost_round=10000, early_stopping_rounds=2000, valid_sets=[val_set], verbose_eval=100)#,\n",
    "                    #feval=lgb_quad_kappa)\n",
    "\n",
    "def make_features(df):\n",
    "    return df.drop([\"installation_id\", \"accuracy_group\"], axis=1), df.accuracy_group.values\n",
    "\n",
    "def make_features_wrapper(train, test):\n",
    "    def make_features(df):\n",
    "        return df.drop([\"installation_id\", \"accuracy_group\"], axis=1), df.accuracy_group.values\n",
    "    \n",
    "    return make_features(train), make_features(test) \n",
    "\n",
    "\n",
    "def make_predictions(model,x_test_all,y_test):\n",
    "    preds=model.predict(x_test_all)\n",
    "    #preds = np.rint(preds)\n",
    "    #preds = np.maximum(0, preds)\n",
    "    #preds = np.minimum(3, preds)\n",
    "    return preds,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_installations(test):\n",
    "    test = test.sort_values(\"timestamp\")\n",
    "    test=test.groupby(\"installation_id\").progress_apply(process_log).reset_index()\n",
    "    test.columns = [\"installation_id\", \"features\"]\n",
    "    result = []\n",
    "    for i, installation_id, feature in test.itertuples():\n",
    "        result.append(feature)\n",
    "        feature[\"installation_id\"]=installation_id\n",
    "    return pd.DataFrame(result).fillna(-1)\n",
    "test_features=process_test_installations(test)\n",
    "\n",
    "for useful_feature in useful_features:\n",
    "    if useful_feature not in test_features.columns:\n",
    "        test_features[useful_feature]=-1\n",
    "        print(\"Missing feature\", useful_feature)\n",
    "\n",
    "test_features=test_features[[c for c in useful_features if c != \"accuracy_group\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../dsb2019/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dsb2019/models/regression_baseline_adv_params.json\", \"r\") as f:\n",
    "    validator_params=json.load(f)\n",
    "#validator_paramstor=best_params\n",
    "selected_features = [f for f in train_features.columns if f not in (\"installation_id\", \"game_session\", \"accuracy_group\")]\n",
    "validator = adv_validation.AdversarialValidator(validator_params, train_features, test_features, selected_features=selected_features,test_size=0.5)\n",
    "validator.fit()\n",
    "print(validator.roc_auc())\n",
    "validator.shap_important_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrain_installations=pd.Series(train_features.installation_id.unique()).sample(frac=1., random_state=2019)\n",
    "subtrain_features=train_features[train_features.installation_id.isin(subtrain_installations.values)].copy()\n",
    "def check_hyperparams(params):\n",
    "    print(params)\n",
    "    if \"max_depth\" in params:\n",
    "        params[\"max_depth\"] = int(params[\"max_depth\"])\n",
    "    if \"num_leaves\" in params:\n",
    "        params[\"num_leaves\"] = int(params[\"num_leaves\"])\n",
    "\n",
    "    train_baseline_with_params = partial(train_baseline, params=params)\n",
    "    cv=InstallationFold(n_splits=3)\n",
    "    predictions = cross_validate(subtrain_features, subtrain_features.accuracy_group, make_features_wrapper, train_baseline_with_params, make_predictions,\n",
    "                                cv=cv)\n",
    "    return {\n",
    "        \"loss\": np.mean([mean_squared_error(true, pred) for pred, true in predictions]),\n",
    "        \"status\": STATUS_OK,\n",
    "        \"params\": params\n",
    "    }\n",
    "\n",
    "\n",
    "def tune(check_params, n_tries=25, n_learning_rate_tries=15, learning_rate=None, n_estimators=10_000):        \n",
    "    if learning_rate is None:\n",
    "        learning_rate_space = {\n",
    "            'learning_rate': hp.loguniform(\"learning_rate\", np.log(0.005), np.log(0.3)),\n",
    "            'metric': 'rmse',\n",
    "            'objective': 'rmse',\n",
    "            #'num_classes': 4,\n",
    "            'random_state': 2019,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"forced_splits\": \"forced_splits.json\",\n",
    "            #\"forcedbins_filename\": \"forced_bins.json\",\n",
    "\n",
    "        }\n",
    "        trials = Trials()\n",
    "        result = fmin(check_params,\n",
    "                      learning_rate_space, tpe.suggest, n_learning_rate_tries, trials=trials)\n",
    "        print(result)\n",
    "        learning_rate = round(trials.best_trial[\"result\"][\"params\"][\"learning_rate\"], 3)\n",
    "\n",
    "    param_space = {\n",
    "        'metric': 'rmse',\n",
    "        'objective': 'rmse',\n",
    "        #'num_classes': 4,\n",
    "        'lambda_l1': hp.uniform(\"lamba_l1\", 1e-10, 1),\n",
    "        'lambda_l2': hp.uniform(\"lambda_l2\", 1e-10, 1),\n",
    "        'random_state': 2019,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_depth\": hp.quniform(\"max_depth\", 3, 16, 1),\n",
    "        \"num_leaves\": hp.choice(\"num_leaves\", [5, 7, 15, 31, 63, 127, 255, 511, 1023, 2047, 4095]),\n",
    "        \"subsample\": hp.quniform(\"subsample\", 0.01, 1, 0.01),\n",
    "        \"feature_fraction\": hp.quniform(\"feature_fraction\", 0.01, 1, 0.01),\n",
    "        \"forced_splits\": \"forced_splits.json\",\n",
    "        #\"forcedbins_filename\": \"forced_bins.json\",\n",
    "    }\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin(check_params,\n",
    "         param_space, tpe.suggest, n_tries, trials=trials)\n",
    "    best_params = trials.best_trial[\"result\"][\"params\"]\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=tune(check_hyperparams, n_tries=100, n_learning_rate_tries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[\"max_depth\"]=10\n",
    "best_params[\"num_leaves\"]=63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was selected on 100% of the data\n",
    "\n",
    "```\n",
    "{'feature_fraction': 0.53,\n",
    " 'lambda_l1': 0.922950554822482,\n",
    " 'lambda_l2': 0.835047934936944,\n",
    " 'learning_rate': 0.006,\n",
    " 'max_depth': 11,\n",
    " 'metric': 'rmse',\n",
    " 'n_estimators': 10000,\n",
    " 'num_leaves': 31,\n",
    " 'objective': 'rmse',\n",
    " 'random_state': 2019,\n",
    " 'subsample': 0.9500000000000001}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params={'feature_fraction': 0.58,\n",
    "#  'lambda_l1': 0.45619796864269707,\n",
    "#  'lambda_l2': 0.033257384218246686,\n",
    "#  'learning_rate': 0.007,\n",
    "#  'max_depth': 14,\n",
    "#  'metric': 'multiclass',\n",
    "#  'n_estimators': 10000,\n",
    "#  'num_classes': 4,\n",
    "#  'num_leaves': 31,\n",
    "#  'objective': 'multiclass',\n",
    "#  'random_state': 2019,\n",
    "#  'subsample': 0.9500000000000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dsb2019/models/regression_baseline_splits_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\tvalid_0's rmse: 5.50926e+38\n",
      "[200]\tvalid_0's rmse: 5.50926e+38\n",
      "[300]\tvalid_0's rmse: 5.50926e+38\n",
      "[400]\tvalid_0's rmse: 5.50926e+38\n",
      "[500]\tvalid_0's rmse: 5.50926e+38\n",
      "[600]\tvalid_0's rmse: 5.50926e+38\n",
      "[700]\tvalid_0's rmse: 5.50926e+38\n",
      "[800]\tvalid_0's rmse: 5.50926e+38\n",
      "[900]\tvalid_0's rmse: 5.50926e+38\n",
      "[1000]\tvalid_0's rmse: 5.50926e+38\n",
      "[1100]\tvalid_0's rmse: 5.50926e+38\n",
      "[1200]\tvalid_0's rmse: 5.50926e+38\n",
      "[1300]\tvalid_0's rmse: 5.50926e+38\n",
      "[1400]\tvalid_0's rmse: 5.50926e+38\n",
      "[1500]\tvalid_0's rmse: 5.50926e+38\n",
      "[1600]\tvalid_0's rmse: 5.50926e+38\n",
      "[1700]\tvalid_0's rmse: 5.50926e+38\n",
      "[1800]\tvalid_0's rmse: 5.50926e+38\n",
      "[1900]\tvalid_0's rmse: 5.50926e+38\n",
      "[2000]\tvalid_0's rmse: 5.50926e+38\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's rmse: 33.8827\n"
     ]
    }
   ],
   "source": [
    "baseline_model=train_baseline(train_features.drop([\"installation_id\", \"accuracy_group\"], axis=1), train_features.accuracy_group.values, \n",
    "               params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\tvalid_0's rmse: 1.34766\n",
      "[200]\tvalid_0's rmse: 2.03436\n",
      "[300]\tvalid_0's rmse: 1.02186\n",
      "[400]\tvalid_0's rmse: 0.991171\n",
      "[500]\tvalid_0's rmse: 1.16942\n",
      "[600]\tvalid_0's rmse: 1.0216\n",
      "[700]\tvalid_0's rmse: 1.03774\n",
      "[800]\tvalid_0's rmse: 1.21829\n",
      "[900]\tvalid_0's rmse: 1.03588\n",
      "[1000]\tvalid_0's rmse: 1.00013\n",
      "[1100]\tvalid_0's rmse: 1.01633\n",
      "[1200]\tvalid_0's rmse: 1.0313\n",
      "[1300]\tvalid_0's rmse: 0.999637\n",
      "[1400]\tvalid_0's rmse: 1.00326\n",
      "[1500]\tvalid_0's rmse: 1.005\n",
      "[1600]\tvalid_0's rmse: 1.00474\n",
      "[1700]\tvalid_0's rmse: 1.02449\n",
      "[1800]\tvalid_0's rmse: 1.00784\n",
      "[1900]\tvalid_0's rmse: 1.00841\n",
      "[2000]\tvalid_0's rmse: 1.0138\n",
      "[2100]\tvalid_0's rmse: 1.01288\n",
      "[2200]\tvalid_0's rmse: 1.01316\n",
      "[2300]\tvalid_0's rmse: 1.017\n",
      "Early stopping, best iteration is:\n",
      "[368]\tvalid_0's rmse: 0.990352\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\tvalid_0's rmse: 1.47953\n",
      "[200]\tvalid_0's rmse: 0.995861\n",
      "[300]\tvalid_0's rmse: 1.02412\n",
      "[400]\tvalid_0's rmse: 2.04913\n",
      "[500]\tvalid_0's rmse: 0.996886\n",
      "[600]\tvalid_0's rmse: 1.00812\n",
      "[700]\tvalid_0's rmse: 2.2197\n",
      "[800]\tvalid_0's rmse: 1.35539\n",
      "[900]\tvalid_0's rmse: 1.09461\n",
      "[1000]\tvalid_0's rmse: 1.17171\n",
      "[1100]\tvalid_0's rmse: 1.00531\n",
      "[1200]\tvalid_0's rmse: 1.72372\n",
      "[1300]\tvalid_0's rmse: 1.62511\n",
      "[1400]\tvalid_0's rmse: 1.15115\n",
      "[1500]\tvalid_0's rmse: 1.11522\n",
      "[1600]\tvalid_0's rmse: 1.08678\n",
      "[1700]\tvalid_0's rmse: 1.11751\n",
      "[1800]\tvalid_0's rmse: 1.08452\n",
      "[1900]\tvalid_0's rmse: 1.11646\n",
      "[2000]\tvalid_0's rmse: 1.09583\n",
      "[2100]\tvalid_0's rmse: 1.08409\n",
      "[2200]\tvalid_0's rmse: 1.08846\n",
      "Early stopping, best iteration is:\n",
      "[294]\tvalid_0's rmse: 0.982919\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\tvalid_0's rmse: 1.36082\n",
      "[200]\tvalid_0's rmse: 1.86591\n",
      "[300]\tvalid_0's rmse: 0.972642\n",
      "[400]\tvalid_0's rmse: 0.96816\n",
      "[500]\tvalid_0's rmse: 1.28411\n",
      "[600]\tvalid_0's rmse: 0.972768\n",
      "[700]\tvalid_0's rmse: 0.983538\n",
      "[800]\tvalid_0's rmse: 1.10969\n",
      "[900]\tvalid_0's rmse: 0.981002\n",
      "[1000]\tvalid_0's rmse: 0.98058\n",
      "[1100]\tvalid_0's rmse: 0.989336\n",
      "[1200]\tvalid_0's rmse: 0.983095\n",
      "[1300]\tvalid_0's rmse: 0.985672\n",
      "[1400]\tvalid_0's rmse: 0.989645\n",
      "[1500]\tvalid_0's rmse: 0.989858\n",
      "[1600]\tvalid_0's rmse: 0.988954\n",
      "[1700]\tvalid_0's rmse: 0.994236\n",
      "[1800]\tvalid_0's rmse: 0.992968\n",
      "[1900]\tvalid_0's rmse: 0.995711\n",
      "[2000]\tvalid_0's rmse: 1.06695\n",
      "[2100]\tvalid_0's rmse: 1.00002\n",
      "[2200]\tvalid_0's rmse: 1.0238\n",
      "[2300]\tvalid_0's rmse: 1.01428\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's rmse: 0.966757\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\tvalid_0's rmse: 0.975572\n",
      "[200]\tvalid_0's rmse: 1.1596\n",
      "[300]\tvalid_0's rmse: 0.972496\n",
      "[400]\tvalid_0's rmse: 0.977359\n",
      "[500]\tvalid_0's rmse: 0.964568\n",
      "[600]\tvalid_0's rmse: 0.968552\n",
      "[700]\tvalid_0's rmse: 1.11653\n",
      "[800]\tvalid_0's rmse: 1.20002\n",
      "[900]\tvalid_0's rmse: 0.980167\n",
      "[1000]\tvalid_0's rmse: 0.97283\n",
      "[1100]\tvalid_0's rmse: 0.972316\n",
      "[1200]\tvalid_0's rmse: 2.33424\n",
      "[1300]\tvalid_0's rmse: 1.09328\n",
      "[1400]\tvalid_0's rmse: 1.00387\n",
      "[1500]\tvalid_0's rmse: 0.978202\n",
      "[1600]\tvalid_0's rmse: 0.980811\n",
      "[1700]\tvalid_0's rmse: 1.15122\n",
      "[1800]\tvalid_0's rmse: 1.0008\n",
      "[1900]\tvalid_0's rmse: 0.993704\n",
      "[2000]\tvalid_0's rmse: 0.979834\n",
      "[2100]\tvalid_0's rmse: 0.980683\n",
      "[2200]\tvalid_0's rmse: 1.01003\n",
      "[2300]\tvalid_0's rmse: 0.996718\n",
      "[2400]\tvalid_0's rmse: 0.983307\n",
      "Early stopping, best iteration is:\n",
      "[479]\tvalid_0's rmse: 0.964354\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\tvalid_0's rmse: 1.00432\n",
      "[200]\tvalid_0's rmse: 1.05222\n",
      "[300]\tvalid_0's rmse: 0.998283\n",
      "[400]\tvalid_0's rmse: 0.998792\n",
      "[500]\tvalid_0's rmse: 1.02717\n",
      "[600]\tvalid_0's rmse: 1.0043\n",
      "[700]\tvalid_0's rmse: 1.00603\n",
      "[800]\tvalid_0's rmse: 1.00462\n",
      "[900]\tvalid_0's rmse: 1.08602\n",
      "[1000]\tvalid_0's rmse: 2.48548\n",
      "[1100]\tvalid_0's rmse: 4.34926\n",
      "[1200]\tvalid_0's rmse: 1.23131\n",
      "[1300]\tvalid_0's rmse: 1.06702\n",
      "[1400]\tvalid_0's rmse: 1.0548\n",
      "[1500]\tvalid_0's rmse: 1.04704\n",
      "[1600]\tvalid_0's rmse: 1.06393\n",
      "[1700]\tvalid_0's rmse: 1.28309\n",
      "[1800]\tvalid_0's rmse: 1.04598\n",
      "[1900]\tvalid_0's rmse: 1.05211\n",
      "[2000]\tvalid_0's rmse: 1.04852\n",
      "[2100]\tvalid_0's rmse: 1.05025\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's rmse: 0.995585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0244837466871028,\n",
       " [1.0360890255297464,\n",
       "  1.0147004267897548,\n",
       "  0.980720702954557,\n",
       "  1.018300951919899,\n",
       "  1.0726076262415578])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cross_validate(train_features, train_features.accuracy_group, make_features_wrapper, partial(train_baseline, params=best_params), \n",
    "                             make_predictions)\n",
    "np.mean([mean_squared_error(true, pred) for pred, true in predictions]), [mean_squared_error(true, pred) for pred, true in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.save_model(str(MODELS_DIR / \"regression_baseline_splits.lgb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = make_features(train_features)\n",
    "prediction=baseline_model.predict(features)\n",
    "clf = ThresholdClassifier()\n",
    "clf.fit(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[0.24199371991009921, 1.8860933030360598, 37.21993309156089]"
      ],
      "text/plain": [
       "[0.24199371991009921, 1.8860933030360598, 37.21993309156089]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
