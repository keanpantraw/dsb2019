{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code/dsb2019/notebooks\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def read_data_model():\n",
    "    if \"LOCAL_LAB_ENV\" in os.environ:\n",
    "        data_dir = Path(\"../data/raw\")\n",
    "        model_dir = Path(\"/code/dsb2019/models\")\n",
    "    else:\n",
    "        data_dir = Path(\"/kaggle/input/data-science-bowl-2019\")\n",
    "        model_dir = Path(\"/kaggle/input/time-baseline3\")\n",
    "\n",
    "    test = pd.read_csv(data_dir / \"test.csv\")\n",
    "    test[\"accuracy_group\"] = None\n",
    "    model = lgb.Booster(model_file=str(model_dir / \"time_baseline.lgb\"))\n",
    "    return test, model\n",
    "\n",
    "test, model = read_data_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lgb_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from dsb2019.models.tracking import track_experiment, track_submission_info\n",
    "from dsb2019.data.validation import InstallationFold, cross_validate, quad_kappa\n",
    "\n",
    "\n",
    "def lgb_quad_kappa(preds, true):\n",
    "    true = true.get_label()\n",
    "    preds = preds.reshape((4, -1)).argmax(axis=0)\n",
    "    return \"quad_kappa\", quad_kappa(true, preds), True\n",
    "    \n",
    "    \n",
    "def train_baseline(x_train,y_train, params=None):\n",
    "    x_train_all, x_val_all,y_train_all,y_val_all = train_test_split(\n",
    "        x_train,y_train,\n",
    "        test_size=0.15,\n",
    "        random_state=2019,\n",
    "    )\n",
    "    train_set = lgb.Dataset(x_train_all, y_train_all)\n",
    "    val_set = lgb.Dataset(x_val_all, y_val_all)\n",
    "\n",
    "    return lgb.train(params, train_set, num_boost_round=10000, early_stopping_rounds=2000, valid_sets=[train_set, val_set], verbose_eval=100,\n",
    "                    feval=lgb_quad_kappa)\n",
    "\n",
    "\n",
    "def make_features_wrapper(*dataframes):\n",
    "    def make_features(df):\n",
    "        return df.drop([\"installation_id\", \"accuracy_group\", \"target_game_session\"], axis=1), df.accuracy_group.values\n",
    "    \n",
    "    result = tuple([make_features(df) for df in dataframes]) \n",
    "    if len(result) == 1:\n",
    "        return result[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def make_predictions(model,x_test_all,y_test):\n",
    "    pred=model.predict(x_test_all).argmax(axis=1)\n",
    "    return pred,y_test\n",
    "\n",
    "\n",
    "def make_submission(test_features, model):\n",
    "    installations = test_features.installation_id.values\n",
    "    test, _ = make_features_wrapper(test_features)\n",
    "    predictions, _ = make_predictions(model, test, None)\n",
    "    return pd.DataFrame(data={\"installation_id\": installations, \"accuracy_group\": predictions})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## game.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = ['Scrub-A-Dub', 'All Star Sorting', 'Mushroom Sorter (Assessment)',\n",
    "       'Air Show', 'Crystals Rule', 'Bird Measurer (Assessment)',\n",
    "       'Dino Drink', 'Bubble Bath', 'Dino Dive', 'Chow Time',\n",
    "       'Cauldron Filler (Assessment)', 'Pan Balance', 'Happy Camel',\n",
    "       'Cart Balancer (Assessment)', 'Chest Sorter (Assessment)',\n",
    "       'Leaf Leader']\n",
    "\n",
    "\n",
    "def calculate_ratios(df):\n",
    "    n_correct=df.correct_move.sum()\n",
    "    n_incorrect=df.wrong_move.sum()\n",
    "    ratio=n_correct/(n_correct+n_incorrect)\n",
    "    return n_correct, n_incorrect, ratio\n",
    "\n",
    "\n",
    "def assessment_title(df, assessment):\n",
    "    assessment_title=assessment.title    \n",
    "    return {\"title\": games.index(assessment_title)}\n",
    "\n",
    "\n",
    "def make_move_stats(df, assessment, title=\"\", n_lags=2):\n",
    "    if \"correct\" in df.columns:\n",
    "        df[\"correct_move\"] = df.correct == True\n",
    "        df[\"wrong_move\"] = df.correct == False\n",
    "    else:\n",
    "        df[\"correct_move\"]=False\n",
    "        df[\"wrong_move\"]=False\n",
    "    result = []\n",
    "    result.extend(zip([f\"n_correct {title}\", f\"n_incorrect {title}\", f\"global_ratio {title}\"], calculate_ratios(df)))\n",
    "    if n_lags:\n",
    "        last_sessions = df.game_session.unique()[-n_lags:]\n",
    "        for i in range(n_lags):\n",
    "            if i < len(last_sessions): \n",
    "                result.extend(zip([f\"n_correct {title} {i}\", f\"n_incorrect {title} {i}\",f\"ratio {title} {i}\"], \n",
    "                                    calculate_ratios(df[df.game_session==last_sessions[i]])))\n",
    "            else:\n",
    "                result.extend(zip([f\"n_correct {title} {i}\", f\"n_incorrect {title} {i}\",f\"ratio {title} {i}\"], [None, None, None]))\n",
    "    return {k: v for k, v in result}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## framework.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import joblib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import reduce, partial\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dsb2019.features.game import games\n",
    "\n",
    "\n",
    "def unwrap_event_data(df):\n",
    "    unwrapped=pd.DataFrame(data=list(df.event_data.apply(json.loads).values))\n",
    "    return pd.concat([unwrapped.reset_index(),df.reset_index()],axis=1)\n",
    "\n",
    "\n",
    "def process_test_installations(test, process_log):\n",
    "    test_labels = prepare_test_labels(test)\n",
    "    return process_installations(test_labels, test, process_log)\n",
    "\n",
    "\n",
    "def process_test_installations_single(test, process_log):\n",
    "    test_labels = prepare_test_labels(test)\n",
    "    return process_installations_single(test_labels, test, process_log)\n",
    "\n",
    "\n",
    "def prepare_test_labels(test):\n",
    "    if \"accuracy_group\" not in test:\n",
    "        test[\"accuracy_group\"] = None\n",
    "    index = {}\n",
    "    df = test[[\"installation_id\", \"game_session\", \"timestamp\"]].sort_values([\"installation_id\", \"timestamp\"])\n",
    "    for i, installation_id, game_session, timestamp in df.itertuples():\n",
    "        index[installation_id] = game_session\n",
    "    installations, sessions = zip(*index.items())\n",
    "    index = pd.DataFrame(data={\"installation_id\": installations, \"game_session\": sessions})\n",
    "    return pd.merge(index, test[[\"game_session\", \"title\", \"installation_id\", \"accuracy_group\"]].drop_duplicates(), \n",
    "                    on=[\"installation_id\", \"game_session\"])\n",
    "\n",
    "\n",
    "def process_installations(train_labels, train, process_log, n_installations_in_chunk=100, n_jobs=None):\n",
    "    installation_ids = train.installation_id.unique()\n",
    "    chunk_size = n_installations_in_chunk\n",
    "    n_jobs = n_jobs if n_jobs is not None else cpu_count()\n",
    "    tasks = []\n",
    "    for p, i_low in enumerate(tqdm(range(0, len(installation_ids), chunk_size), desc=\"Generating tasks\", position=0)):\n",
    "        i_high = min(len(installation_ids), i_low + chunk_size)\n",
    "        installation_ids_chunk = installation_ids[i_low:i_high]\n",
    "        train_labels_chunk = train_labels[train_labels.installation_id.isin(installation_ids_chunk)].copy()\n",
    "        train_chunk = train[train.installation_id.isin(installation_ids_chunk)].copy()\n",
    "        task = joblib.delayed(process_installations_single)(train_labels_chunk, train_chunk, process_log, position=p+1)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    result = []\n",
    "    with tqdm_joblib(tqdm(desc=\"Completing tasks\", total=len(tasks), position=0)) as progress_bar:\n",
    "        with joblib.Parallel(n_jobs=n_jobs) as workers:\n",
    "            for result_df in workers(tasks):\n",
    "                result.append(result_df)\n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback:\n",
    "        def __init__(self, time, index, parallel):\n",
    "            self.index = index\n",
    "            self.parallel = parallel\n",
    "\n",
    "        def __call__(self, index):\n",
    "            tqdm_object.update()\n",
    "            if self.parallel._original_iterator is not None:\n",
    "                self.parallel.dispatch_next()\n",
    "    \n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def process_installations_single(train_labels, train, process_log, position=1):\n",
    "    result = []\n",
    "    train = train.sort_values(\"timestamp\")\n",
    "    train[\"timestamp\"] = pd.to_datetime(train.timestamp)\n",
    "    installations = train.groupby(\"installation_id\")\n",
    "    for i, game_session, title, installation_id, accuracy_group in tqdm(train_labels[[\"game_session\", \"title\", \"installation_id\", \"accuracy_group\"]].itertuples(), \n",
    "                                                              total=len(train_labels), position=position, desc=f\"Processing chunk {position}\"):\n",
    "        player_log = installations.get_group(installation_id).reset_index()\n",
    "        log_length = player_log[(player_log.game_session==game_session) & (player_log.title==title)].index[0]\n",
    "        player_log = player_log.iloc[:(log_length + 1)]\n",
    "        player_log[\"accuracy_group\"] = accuracy_group\n",
    "        player_log[\"target_game_session\"] = game_session\n",
    "        features = process_log(player_log)\n",
    "        features[\"installation_id\"] = installation_id\n",
    "        features[\"target_game_session\"] = game_session\n",
    "        features[\"accuracy_group\"] = accuracy_group\n",
    "        result.append(features)\n",
    "    df = pd.DataFrame(data=result)\n",
    "    return df[sorted(df.columns)].fillna(-1)\n",
    "\n",
    "\n",
    "class LogProcessor:\n",
    "    def __init__(self, global_features, game_features):\n",
    "        self.global_features = global_features\n",
    "        self.game_features = game_features\n",
    "\n",
    "    def __call__(self, df):\n",
    "        assessment = df.iloc[-1]\n",
    "        history = df.iloc[:-1]\n",
    "        history = history[history.type.isin([\"Game\", \"Assessment\"])].copy()\n",
    "        \n",
    "        result = {}\n",
    "        for func in self.global_features:\n",
    "            result.update(func(df, assessment))\n",
    "        for game in games:\n",
    "            game_feature_funcs = self.game_features.get(game, [])\n",
    "            if game_feature_funcs:\n",
    "                game_info=history[history.title==game].copy()\n",
    "                if len(game_info):\n",
    "                    game_info = unwrap_event_data(game_info)\n",
    "                for func in game_feature_funcs:\n",
    "                    result.update(func(game_info, assessment))\n",
    "        return result\n",
    "\n",
    "\n",
    "def with_prefix(func, prefix):\n",
    "    def f(df, assessment):\n",
    "        return {f\"{prefix}_{k}\": v for k, v in func(df, assessment).items()}\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_calendar_features(df, assessment):\n",
    "    ts = assessment.timestamp\n",
    "    year = ts.year\n",
    "    month = ts.month\n",
    "    dayofweek = ts.dayofweek\n",
    "    time = ts.time()\n",
    "    return {\n",
    "        \"month\": month,\n",
    "        \"dayofweek\": dayofweek,\n",
    "        \"hour\": time.hour,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_base_time_features(df, assessment):\n",
    "    start_end_times = df.groupby(\"game_session\").agg({\"timestamp\": [\"min\", \"max\", \"count\"]}).reset_index()\n",
    "    start_end_times.columns = [\"game_session\", \"start_time\", \"end_time\", \"n_turns\"]\n",
    "    start_end_times[\"duration\"] = start_end_times.end_time - start_end_times.start_time\n",
    "    duration_minutes = start_end_times.duration / np.timedelta64(1, \"m\")\n",
    "    result = {\n",
    "        \"mean_session_time_minutes\": round(duration_minutes.mean(), 2), \n",
    "        \"mean_session_turns\":  round(start_end_times.n_turns.mean(), 2)\n",
    "    }\n",
    "    last_event_time = assessment.timestamp\n",
    "    first_event_time = start_end_times.start_time.min()\n",
    "    \n",
    "    days_active = round((last_event_time - first_event_time) / np.timedelta64(1, \"D\"), 0) + 1\n",
    "    result[\"games_per_day\"] = round(df.game_session.nunique() / days_active, 2)\n",
    "    result[\"games_played\"] = df.game_session.nunique()\n",
    "    minutes_between_games = ((start_end_times.start_time - start_end_times.start_time.shift(1)).dropna() / np.timedelta64(1, \"m\")).round(1)\n",
    "    result[\"mean_minutes_between_games\"] = round(minutes_between_games.mean(), 2)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tasks: 100%|██████████| 10/10 [00:00<00:00, 25.88it/s]\n",
      "Completing tasks: 100%|██████████| 10/10 [00:49<00:00,  4.96s/it]\n"
     ]
    }
   ],
   "source": [
    "log_processor = LogProcessor([assessment_title, make_calendar_features, \n",
    "                                                make_base_time_features], \n",
    "                                               {name: [partial(make_move_stats, title=name),\n",
    "                                                       with_prefix(make_base_time_features, name)] \n",
    "                                                for name in games})\n",
    "test_features = process_test_installations(test, log_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = make_submission(test_features, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    776\n",
       "0    195\n",
       "1     29\n",
       "Name: accuracy_group, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.accuracy_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../data/submissions/time_baseline.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
