{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from target_encoding import TargetEncoderClassifier, TargetEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import reduce\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from functools import partial\n",
    "\n",
    "from dsb2019.data.validation import InstallationFold, cross_validate, quad_kappa\n",
    "from dsb2019.data import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world', 'timestamp']\n",
    "\n",
    "train = pd.read_csv(DATA_DIR / 'interim/train.csv', usecols=keep_cols)\n",
    "test = pd.read_csv(DATA_DIR / 'raw/test.csv', usecols=keep_cols)\n",
    "train_labels = pd.read_csv(DATA_DIR / 'raw/train_labels.csv')\n",
    "submission = pd.read_csv(DATA_DIR / 'raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_reduce(df):\n",
    "    # group1 and group2 are intermediary \"game session\" groups,\n",
    "    # which are reduced to one record by game session. group1 takes\n",
    "    # the max value of game_time (final game time in a session) and \n",
    "    # of event_count (total number of events happened in the session).\n",
    "    # group2 takes the total number of event_code of each type\n",
    "    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n",
    "        ['game_session', 'installation_id', 'title', 'type', 'world']\n",
    "    ).max().reset_index()\n",
    "\n",
    "    group2 = pd.get_dummies(\n",
    "        df[['installation_id', 'event_code']], \n",
    "        columns=['event_code']\n",
    "    ).groupby(['installation_id']).sum()\n",
    "\n",
    "    # group3, group4 and group5 are grouped by installation_id \n",
    "    # and reduced using summation and other summary stats\n",
    "    group3 = pd.get_dummies(\n",
    "        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n",
    "        columns=['title', 'type', 'world']\n",
    "    ).groupby(['installation_id']).sum()\n",
    "\n",
    "    group4 = group1[\n",
    "        ['installation_id', 'event_count', 'game_time']\n",
    "    ].groupby(\n",
    "        ['installation_id']\n",
    "    ).agg([np.sum, np.mean, np.std])\n",
    "\n",
    "    return group2.join(group3).join(group4).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_reduce_wrapper(df):\n",
    "    result = group_and_reduce(df)\n",
    "    return result.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "def process_installations(train_labels, train, process_log):\n",
    "    result = []\n",
    "    train=train.sort_values(\"timestamp\")\n",
    "    installations = train.groupby(\"installation_id\")\n",
    "    for i, game_session, title, installation_id, accuracy_group in tqdm(train_labels[[\"game_session\", \"title\", \"installation_id\", \"accuracy_group\"]].itertuples(), \n",
    "                                                              total=len(train_labels)):\n",
    "        player_log = installations.get_group(installation_id).reset_index()\n",
    "        log_length = player_log[(player_log.game_session==game_session) & (player_log.title==title)].index[0]\n",
    "        player_log = player_log.iloc[:(log_length + 1)]\n",
    "        player_log[\"accuracy_group\"] = accuracy_group\n",
    "        player_log[\"target_game_session\"] = game_session\n",
    "        features = process_log(player_log)\n",
    "        features[\"installation_id\"] = installation_id\n",
    "        features[\"accuracy_group\"] = accuracy_group\n",
    "        result.append(features)\n",
    "    return pd.DataFrame(data=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17690/17690 [23:40<00:00, 12.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17690, 103)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>installation_id</th>\n",
       "      <th>event_code_2000</th>\n",
       "      <th>event_code_2010</th>\n",
       "      <th>event_code_2020</th>\n",
       "      <th>event_code_2025</th>\n",
       "      <th>event_code_2030</th>\n",
       "      <th>event_code_2035</th>\n",
       "      <th>event_code_2040</th>\n",
       "      <th>event_code_2050</th>\n",
       "      <th>event_code_2060</th>\n",
       "      <th>...</th>\n",
       "      <th>title_Pan Balance</th>\n",
       "      <th>title_Cart Balancer (Assessment)</th>\n",
       "      <th>title_Chest Sorter (Assessment)</th>\n",
       "      <th>title_Egg Dropper (Activity)</th>\n",
       "      <th>title_Happy Camel</th>\n",
       "      <th>title_Heavy, Heavier, Heaviest</th>\n",
       "      <th>title_Honey Cake</th>\n",
       "      <th>event_code_4050</th>\n",
       "      <th>title_Leaf Leader</th>\n",
       "      <th>event_code_4080</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006a69f</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006a69f</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006a69f</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006a69f</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0006a69f</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  installation_id  event_code_2000  event_code_2010  event_code_2020  \\\n",
       "0        0006a69f             27.0              1.0             27.0   \n",
       "1        0006a69f             26.0              1.0             26.0   \n",
       "2        0006a69f             19.0              NaN             20.0   \n",
       "3        0006a69f             48.0              2.0             52.0   \n",
       "4        0006a69f             57.0              3.0             64.0   \n",
       "\n",
       "   event_code_2025  event_code_2030  event_code_2035  event_code_2040  \\\n",
       "0              5.0             22.0              1.0              6.0   \n",
       "1              5.0             22.0              1.0              6.0   \n",
       "2              4.0             18.0              NaN              6.0   \n",
       "3              9.0             43.0              5.0             10.0   \n",
       "4             10.0             53.0              6.0             10.0   \n",
       "\n",
       "   event_code_2050  event_code_2060  ...  title_Pan Balance  \\\n",
       "0              6.0              1.0  ...                NaN   \n",
       "1              6.0              1.0  ...                NaN   \n",
       "2              6.0              NaN  ...                NaN   \n",
       "3              9.0              2.0  ...                NaN   \n",
       "4              9.0              3.0  ...                NaN   \n",
       "\n",
       "   title_Cart Balancer (Assessment)  title_Chest Sorter (Assessment)  \\\n",
       "0                               NaN                              NaN   \n",
       "1                               NaN                              NaN   \n",
       "2                               NaN                              NaN   \n",
       "3                               NaN                              NaN   \n",
       "4                               NaN                              NaN   \n",
       "\n",
       "   title_Egg Dropper (Activity)  title_Happy Camel  \\\n",
       "0                           NaN                NaN   \n",
       "1                           NaN                NaN   \n",
       "2                           NaN                NaN   \n",
       "3                           NaN                NaN   \n",
       "4                           NaN                NaN   \n",
       "\n",
       "   title_Heavy, Heavier, Heaviest  title_Honey Cake  event_code_4050  \\\n",
       "0                             NaN               NaN              NaN   \n",
       "1                             NaN               NaN              NaN   \n",
       "2                             NaN               NaN              NaN   \n",
       "3                             NaN               NaN              NaN   \n",
       "4                             NaN               NaN              NaN   \n",
       "\n",
       "   title_Leaf Leader  event_code_4080  \n",
       "0                NaN              NaN  \n",
       "1                NaN              NaN  \n",
       "2                NaN              NaN  \n",
       "3                NaN              NaN  \n",
       "4                NaN              NaN  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = process_installations(train_labels, train, group_reduce_wrapper)\n",
    "test = group_and_reduce(test)\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = train_labels[['installation_id', 'accuracy_group']]\n",
    "#train = train.merge(labels, how='left', on='installation_id').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(train, test, alpha=10, max_unique=50):\n",
    "    test = test.drop(\"accuracy_group\", axis=1)\n",
    "    len_uniques = []\n",
    "    train_labeled = train.fillna(-999)\n",
    "    test_labeled = test.fillna(-999)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        train.drop(['installation_id', 'accuracy_group'], axis=1),\n",
    "        train['accuracy_group'],\n",
    "        test_size=0.15,\n",
    "        random_state=2019,\n",
    "    )\n",
    "    \n",
    "    for c in train.columns.drop(['installation_id', 'accuracy_group']):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(pd.concat([train_labeled[c], test_labeled[c]])) \n",
    "        train_labeled[c] = le.transform(train_labeled[c])\n",
    "        test_labeled[c] = le.transform(test_labeled[c])\n",
    "        len_uniques.append(len(le.classes_))\n",
    "\n",
    "    x_train_labeled_ix, x_val_labeled_ix = train_test_split(\n",
    "        np.arange(len(train_labeled)),\n",
    "        test_size=0.15,\n",
    "        random_state=2019,\n",
    "    )\n",
    "    x_train_labeled = train_labeled.drop(['installation_id', 'accuracy_group'], axis=1).iloc[x_train_labeled_ix]\n",
    "    x_val_labeled = train_labeled.drop(['installation_id', 'accuracy_group'], axis=1).iloc[x_val_labeled_ix]\n",
    "    \n",
    "    cv = InstallationFold(train_labeled.installation_id.values[x_train_labeled_ix])\n",
    "\n",
    "    enc = TargetEncoder(alpha=alpha, max_unique=max_unique, split=[cv])\n",
    "    x_train_encoded = enc.transform_train(x_train_labeled, y=y_train)\n",
    "    x_val_encoded = enc.transform_test(x_val_labeled)\n",
    "    x_test_encoded = enc.transform_test(test.drop(['installation_id'], axis=1))\n",
    "\n",
    "    x_train_encoded = pd.DataFrame(x_train_encoded)\n",
    "    x_val_encoded = pd.DataFrame(x_val_encoded)\n",
    "    x_test_encoded = pd.DataFrame(x_test_encoded)\n",
    "\n",
    "    x_train_all = pd.concat([x_train.reset_index(drop=True), x_train_encoded], axis=1)\n",
    "    x_val_all = pd.concat([x_val.reset_index(drop=True), x_val_encoded], axis=1)\n",
    "    x_test_all = pd.concat([test.drop(['installation_id'], axis=1), x_test_encoded], axis=1)\n",
    "\n",
    "    return x_train_all, x_val_all, x_test_all, y_train, y_val\n",
    "\n",
    "\n",
    "def train_baseline(x_train_all,x_val_all,y_train,y_val):\n",
    "    train_set = lgb.Dataset(x_train_all, y_train)\n",
    "    val_set = lgb.Dataset(x_val_all, y_val)\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'feature_fraction': 0.9,\n",
    "        'num_leaves': 14,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 1,\n",
    "        'metric': 'multiclass',\n",
    "        'objective': 'multiclass',\n",
    "        'num_classes': 4,\n",
    "        'random_state': 2019\n",
    "    }\n",
    "\n",
    "    return lgb.train(params, train_set, num_boost_round=10000, early_stopping_rounds=300, valid_sets=[train_set, val_set], verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.14178\tvalid_1's multi_logloss: 1.15511\n",
      "[200]\ttraining's multi_logloss: 1.10206\tvalid_1's multi_logloss: 1.13437\n",
      "[300]\ttraining's multi_logloss: 1.07304\tvalid_1's multi_logloss: 1.12389\n",
      "[400]\ttraining's multi_logloss: 1.04845\tvalid_1's multi_logloss: 1.11767\n",
      "[500]\ttraining's multi_logloss: 1.02734\tvalid_1's multi_logloss: 1.11435\n",
      "[600]\ttraining's multi_logloss: 1.00843\tvalid_1's multi_logloss: 1.11159\n",
      "[700]\ttraining's multi_logloss: 0.990944\tvalid_1's multi_logloss: 1.10999\n",
      "[800]\ttraining's multi_logloss: 0.974612\tvalid_1's multi_logloss: 1.10872\n",
      "[900]\ttraining's multi_logloss: 0.958925\tvalid_1's multi_logloss: 1.1078\n",
      "[1000]\ttraining's multi_logloss: 0.943894\tvalid_1's multi_logloss: 1.10726\n",
      "[1100]\ttraining's multi_logloss: 0.929746\tvalid_1's multi_logloss: 1.10672\n",
      "[1200]\ttraining's multi_logloss: 0.915851\tvalid_1's multi_logloss: 1.10603\n",
      "[1300]\ttraining's multi_logloss: 0.902424\tvalid_1's multi_logloss: 1.10517\n",
      "[1400]\ttraining's multi_logloss: 0.889364\tvalid_1's multi_logloss: 1.10436\n",
      "[1500]\ttraining's multi_logloss: 0.876879\tvalid_1's multi_logloss: 1.10335\n",
      "[1600]\ttraining's multi_logloss: 0.864727\tvalid_1's multi_logloss: 1.10285\n",
      "[1700]\ttraining's multi_logloss: 0.853044\tvalid_1's multi_logloss: 1.10232\n",
      "[1800]\ttraining's multi_logloss: 0.841497\tvalid_1's multi_logloss: 1.10201\n",
      "[1900]\ttraining's multi_logloss: 0.8303\tvalid_1's multi_logloss: 1.10183\n",
      "[2000]\ttraining's multi_logloss: 0.819527\tvalid_1's multi_logloss: 1.10167\n",
      "[2100]\ttraining's multi_logloss: 0.808738\tvalid_1's multi_logloss: 1.10161\n",
      "[2200]\ttraining's multi_logloss: 0.798208\tvalid_1's multi_logloss: 1.10143\n",
      "[2300]\ttraining's multi_logloss: 0.78789\tvalid_1's multi_logloss: 1.10144\n",
      "[2400]\ttraining's multi_logloss: 0.777671\tvalid_1's multi_logloss: 1.10119\n",
      "[2500]\ttraining's multi_logloss: 0.767674\tvalid_1's multi_logloss: 1.10111\n",
      "[2600]\ttraining's multi_logloss: 0.75792\tvalid_1's multi_logloss: 1.10134\n",
      "[2700]\ttraining's multi_logloss: 0.748311\tvalid_1's multi_logloss: 1.1017\n",
      "[2800]\ttraining's multi_logloss: 0.738837\tvalid_1's multi_logloss: 1.10215\n",
      "Early stopping, best iteration is:\n",
      "[2512]\ttraining's multi_logloss: 0.766527\tvalid_1's multi_logloss: 1.10107\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.13643\tvalid_1's multi_logloss: 1.17638\n",
      "[200]\ttraining's multi_logloss: 1.09524\tvalid_1's multi_logloss: 1.15514\n",
      "[300]\ttraining's multi_logloss: 1.066\tvalid_1's multi_logloss: 1.14452\n",
      "[400]\ttraining's multi_logloss: 1.04164\tvalid_1's multi_logloss: 1.13824\n",
      "[500]\ttraining's multi_logloss: 1.0207\tvalid_1's multi_logloss: 1.13492\n",
      "[600]\ttraining's multi_logloss: 1.00151\tvalid_1's multi_logloss: 1.13288\n",
      "[700]\ttraining's multi_logloss: 0.983881\tvalid_1's multi_logloss: 1.13199\n",
      "[800]\ttraining's multi_logloss: 0.967519\tvalid_1's multi_logloss: 1.13075\n",
      "[900]\ttraining's multi_logloss: 0.952192\tvalid_1's multi_logloss: 1.13032\n",
      "[1000]\ttraining's multi_logloss: 0.937566\tvalid_1's multi_logloss: 1.12968\n",
      "[1100]\ttraining's multi_logloss: 0.923713\tvalid_1's multi_logloss: 1.129\n",
      "[1200]\ttraining's multi_logloss: 0.910314\tvalid_1's multi_logloss: 1.12857\n",
      "[1300]\ttraining's multi_logloss: 0.897358\tvalid_1's multi_logloss: 1.12842\n",
      "[1400]\ttraining's multi_logloss: 0.884956\tvalid_1's multi_logloss: 1.12844\n",
      "[1500]\ttraining's multi_logloss: 0.872804\tvalid_1's multi_logloss: 1.12837\n",
      "[1600]\ttraining's multi_logloss: 0.860755\tvalid_1's multi_logloss: 1.12829\n",
      "[1700]\ttraining's multi_logloss: 0.849042\tvalid_1's multi_logloss: 1.12814\n",
      "[1800]\ttraining's multi_logloss: 0.837513\tvalid_1's multi_logloss: 1.12818\n",
      "[1900]\ttraining's multi_logloss: 0.826315\tvalid_1's multi_logloss: 1.12818\n",
      "[2000]\ttraining's multi_logloss: 0.815246\tvalid_1's multi_logloss: 1.12801\n",
      "[2100]\ttraining's multi_logloss: 0.804481\tvalid_1's multi_logloss: 1.12827\n",
      "[2200]\ttraining's multi_logloss: 0.793775\tvalid_1's multi_logloss: 1.12836\n",
      "Early stopping, best iteration is:\n",
      "[1986]\ttraining's multi_logloss: 0.81679\tvalid_1's multi_logloss: 1.1279\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.13691\tvalid_1's multi_logloss: 1.15993\n",
      "[200]\ttraining's multi_logloss: 1.09516\tvalid_1's multi_logloss: 1.13723\n",
      "[300]\ttraining's multi_logloss: 1.06478\tvalid_1's multi_logloss: 1.12697\n",
      "[400]\ttraining's multi_logloss: 1.03985\tvalid_1's multi_logloss: 1.12047\n",
      "[500]\ttraining's multi_logloss: 1.01834\tvalid_1's multi_logloss: 1.11733\n",
      "[600]\ttraining's multi_logloss: 0.998715\tvalid_1's multi_logloss: 1.11483\n",
      "[700]\ttraining's multi_logloss: 0.980897\tvalid_1's multi_logloss: 1.11322\n",
      "[800]\ttraining's multi_logloss: 0.964195\tvalid_1's multi_logloss: 1.1119\n",
      "[900]\ttraining's multi_logloss: 0.948684\tvalid_1's multi_logloss: 1.11143\n",
      "[1000]\ttraining's multi_logloss: 0.933964\tvalid_1's multi_logloss: 1.11058\n",
      "[1100]\ttraining's multi_logloss: 0.91986\tvalid_1's multi_logloss: 1.11009\n",
      "[1200]\ttraining's multi_logloss: 0.906335\tvalid_1's multi_logloss: 1.10989\n",
      "[1300]\ttraining's multi_logloss: 0.893305\tvalid_1's multi_logloss: 1.10954\n",
      "[1400]\ttraining's multi_logloss: 0.880403\tvalid_1's multi_logloss: 1.10918\n",
      "[1500]\ttraining's multi_logloss: 0.867997\tvalid_1's multi_logloss: 1.10908\n",
      "[1600]\ttraining's multi_logloss: 0.855771\tvalid_1's multi_logloss: 1.10868\n",
      "[1700]\ttraining's multi_logloss: 0.843845\tvalid_1's multi_logloss: 1.10839\n",
      "[1800]\ttraining's multi_logloss: 0.832282\tvalid_1's multi_logloss: 1.10848\n",
      "[1900]\ttraining's multi_logloss: 0.821231\tvalid_1's multi_logloss: 1.1083\n",
      "[2000]\ttraining's multi_logloss: 0.810377\tvalid_1's multi_logloss: 1.10813\n",
      "[2100]\ttraining's multi_logloss: 0.799807\tvalid_1's multi_logloss: 1.10797\n",
      "[2200]\ttraining's multi_logloss: 0.789279\tvalid_1's multi_logloss: 1.1081\n",
      "[2300]\ttraining's multi_logloss: 0.779016\tvalid_1's multi_logloss: 1.10821\n",
      "[2400]\ttraining's multi_logloss: 0.768784\tvalid_1's multi_logloss: 1.10847\n",
      "Early stopping, best iteration is:\n",
      "[2103]\ttraining's multi_logloss: 0.799492\tvalid_1's multi_logloss: 1.10792\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.14165\tvalid_1's multi_logloss: 1.16092\n",
      "[200]\ttraining's multi_logloss: 1.10105\tvalid_1's multi_logloss: 1.14083\n",
      "[300]\ttraining's multi_logloss: 1.07161\tvalid_1's multi_logloss: 1.13122\n",
      "[400]\ttraining's multi_logloss: 1.04725\tvalid_1's multi_logloss: 1.12601\n",
      "[500]\ttraining's multi_logloss: 1.02579\tvalid_1's multi_logloss: 1.12218\n",
      "[600]\ttraining's multi_logloss: 1.00623\tvalid_1's multi_logloss: 1.12019\n",
      "[700]\ttraining's multi_logloss: 0.988109\tvalid_1's multi_logloss: 1.119\n",
      "[800]\ttraining's multi_logloss: 0.97133\tvalid_1's multi_logloss: 1.11814\n",
      "[900]\ttraining's multi_logloss: 0.95542\tvalid_1's multi_logloss: 1.11756\n",
      "[1000]\ttraining's multi_logloss: 0.940372\tvalid_1's multi_logloss: 1.1169\n",
      "[1100]\ttraining's multi_logloss: 0.926163\tvalid_1's multi_logloss: 1.11645\n",
      "[1200]\ttraining's multi_logloss: 0.912544\tvalid_1's multi_logloss: 1.11612\n",
      "[1300]\ttraining's multi_logloss: 0.899302\tvalid_1's multi_logloss: 1.11577\n",
      "[1400]\ttraining's multi_logloss: 0.886634\tvalid_1's multi_logloss: 1.11578\n",
      "[1500]\ttraining's multi_logloss: 0.874324\tvalid_1's multi_logloss: 1.11585\n",
      "[1600]\ttraining's multi_logloss: 0.862229\tvalid_1's multi_logloss: 1.11622\n",
      "Early stopping, best iteration is:\n",
      "[1382]\ttraining's multi_logloss: 0.888913\tvalid_1's multi_logloss: 1.11571\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.14136\tvalid_1's multi_logloss: 1.16932\n",
      "[200]\ttraining's multi_logloss: 1.10029\tvalid_1's multi_logloss: 1.15019\n",
      "[300]\ttraining's multi_logloss: 1.07052\tvalid_1's multi_logloss: 1.14103\n",
      "[400]\ttraining's multi_logloss: 1.04573\tvalid_1's multi_logloss: 1.13498\n",
      "[500]\ttraining's multi_logloss: 1.02399\tvalid_1's multi_logloss: 1.13063\n",
      "[600]\ttraining's multi_logloss: 1.00448\tvalid_1's multi_logloss: 1.12728\n",
      "[700]\ttraining's multi_logloss: 0.986754\tvalid_1's multi_logloss: 1.12521\n",
      "[800]\ttraining's multi_logloss: 0.970431\tvalid_1's multi_logloss: 1.12365\n",
      "[900]\ttraining's multi_logloss: 0.955007\tvalid_1's multi_logloss: 1.1223\n",
      "[1000]\ttraining's multi_logloss: 0.940328\tvalid_1's multi_logloss: 1.12127\n",
      "[1100]\ttraining's multi_logloss: 0.926153\tvalid_1's multi_logloss: 1.12057\n",
      "[1200]\ttraining's multi_logloss: 0.912507\tvalid_1's multi_logloss: 1.11984\n",
      "[1300]\ttraining's multi_logloss: 0.899258\tvalid_1's multi_logloss: 1.11952\n",
      "[1400]\ttraining's multi_logloss: 0.886444\tvalid_1's multi_logloss: 1.11911\n",
      "[1500]\ttraining's multi_logloss: 0.87408\tvalid_1's multi_logloss: 1.11908\n",
      "[1600]\ttraining's multi_logloss: 0.862076\tvalid_1's multi_logloss: 1.11897\n",
      "[1700]\ttraining's multi_logloss: 0.850361\tvalid_1's multi_logloss: 1.11892\n",
      "[1800]\ttraining's multi_logloss: 0.838957\tvalid_1's multi_logloss: 1.1191\n",
      "[1900]\ttraining's multi_logloss: 0.827892\tvalid_1's multi_logloss: 1.11914\n",
      "Early stopping, best iteration is:\n",
      "[1671]\ttraining's multi_logloss: 0.853768\tvalid_1's multi_logloss: 1.11888\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.13748\tvalid_1's multi_logloss: 1.1764\n",
      "[200]\ttraining's multi_logloss: 1.09671\tvalid_1's multi_logloss: 1.15648\n",
      "[300]\ttraining's multi_logloss: 1.06673\tvalid_1's multi_logloss: 1.14548\n",
      "[400]\ttraining's multi_logloss: 1.04253\tvalid_1's multi_logloss: 1.13921\n",
      "[500]\ttraining's multi_logloss: 1.02138\tvalid_1's multi_logloss: 1.1355\n",
      "[600]\ttraining's multi_logloss: 1.00231\tvalid_1's multi_logloss: 1.1335\n",
      "[700]\ttraining's multi_logloss: 0.984948\tvalid_1's multi_logloss: 1.13212\n",
      "[800]\ttraining's multi_logloss: 0.96902\tvalid_1's multi_logloss: 1.1313\n",
      "[900]\ttraining's multi_logloss: 0.953732\tvalid_1's multi_logloss: 1.13096\n",
      "[1000]\ttraining's multi_logloss: 0.939244\tvalid_1's multi_logloss: 1.13088\n",
      "[1100]\ttraining's multi_logloss: 0.925174\tvalid_1's multi_logloss: 1.13047\n",
      "[1200]\ttraining's multi_logloss: 0.911676\tvalid_1's multi_logloss: 1.12997\n",
      "[1300]\ttraining's multi_logloss: 0.898617\tvalid_1's multi_logloss: 1.12952\n",
      "[1400]\ttraining's multi_logloss: 0.886046\tvalid_1's multi_logloss: 1.12957\n",
      "[1500]\ttraining's multi_logloss: 0.873544\tvalid_1's multi_logloss: 1.12962\n",
      "Early stopping, best iteration is:\n",
      "[1291]\ttraining's multi_logloss: 0.899782\tvalid_1's multi_logloss: 1.12945\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.14165\tvalid_1's multi_logloss: 1.16017\n",
      "[200]\ttraining's multi_logloss: 1.10042\tvalid_1's multi_logloss: 1.14017\n",
      "[300]\ttraining's multi_logloss: 1.07028\tvalid_1's multi_logloss: 1.13012\n",
      "[400]\ttraining's multi_logloss: 1.04585\tvalid_1's multi_logloss: 1.12557\n",
      "[500]\ttraining's multi_logloss: 1.02433\tvalid_1's multi_logloss: 1.1215\n",
      "[600]\ttraining's multi_logloss: 1.00487\tvalid_1's multi_logloss: 1.11891\n",
      "[700]\ttraining's multi_logloss: 0.987353\tvalid_1's multi_logloss: 1.11787\n",
      "[800]\ttraining's multi_logloss: 0.970872\tvalid_1's multi_logloss: 1.11686\n",
      "[900]\ttraining's multi_logloss: 0.955421\tvalid_1's multi_logloss: 1.11631\n",
      "[1000]\ttraining's multi_logloss: 0.94089\tvalid_1's multi_logloss: 1.11596\n",
      "[1100]\ttraining's multi_logloss: 0.92674\tvalid_1's multi_logloss: 1.11596\n",
      "[1200]\ttraining's multi_logloss: 0.913183\tvalid_1's multi_logloss: 1.11563\n",
      "[1300]\ttraining's multi_logloss: 0.899852\tvalid_1's multi_logloss: 1.11521\n",
      "[1400]\ttraining's multi_logloss: 0.886948\tvalid_1's multi_logloss: 1.11471\n",
      "[1500]\ttraining's multi_logloss: 0.874491\tvalid_1's multi_logloss: 1.11426\n",
      "[1600]\ttraining's multi_logloss: 0.862313\tvalid_1's multi_logloss: 1.11354\n",
      "[1700]\ttraining's multi_logloss: 0.850372\tvalid_1's multi_logloss: 1.11297\n",
      "[1800]\ttraining's multi_logloss: 0.838844\tvalid_1's multi_logloss: 1.11272\n",
      "[1900]\ttraining's multi_logloss: 0.827739\tvalid_1's multi_logloss: 1.1127\n",
      "[2000]\ttraining's multi_logloss: 0.816786\tvalid_1's multi_logloss: 1.11249\n",
      "[2100]\ttraining's multi_logloss: 0.80615\tvalid_1's multi_logloss: 1.11263\n",
      "[2200]\ttraining's multi_logloss: 0.795772\tvalid_1's multi_logloss: 1.11298\n",
      "[2300]\ttraining's multi_logloss: 0.785548\tvalid_1's multi_logloss: 1.11316\n",
      "Early stopping, best iteration is:\n",
      "[2057]\ttraining's multi_logloss: 0.810648\tvalid_1's multi_logloss: 1.11246\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.14189\tvalid_1's multi_logloss: 1.14419\n",
      "[200]\ttraining's multi_logloss: 1.10208\tvalid_1's multi_logloss: 1.12324\n",
      "[300]\ttraining's multi_logloss: 1.07278\tvalid_1's multi_logloss: 1.1135\n",
      "[400]\ttraining's multi_logloss: 1.04888\tvalid_1's multi_logloss: 1.10691\n",
      "[500]\ttraining's multi_logloss: 1.02794\tvalid_1's multi_logloss: 1.10355\n",
      "[600]\ttraining's multi_logloss: 1.00879\tvalid_1's multi_logloss: 1.10114\n",
      "[700]\ttraining's multi_logloss: 0.991093\tvalid_1's multi_logloss: 1.0992\n",
      "[800]\ttraining's multi_logloss: 0.974681\tvalid_1's multi_logloss: 1.09772\n",
      "[900]\ttraining's multi_logloss: 0.959168\tvalid_1's multi_logloss: 1.09648\n",
      "[1000]\ttraining's multi_logloss: 0.944586\tvalid_1's multi_logloss: 1.09519\n",
      "[1100]\ttraining's multi_logloss: 0.930385\tvalid_1's multi_logloss: 1.09415\n",
      "[1200]\ttraining's multi_logloss: 0.91689\tvalid_1's multi_logloss: 1.09316\n",
      "[1300]\ttraining's multi_logloss: 0.90388\tvalid_1's multi_logloss: 1.09272\n",
      "[1400]\ttraining's multi_logloss: 0.891143\tvalid_1's multi_logloss: 1.09264\n",
      "[1500]\ttraining's multi_logloss: 0.878955\tvalid_1's multi_logloss: 1.09252\n",
      "[1600]\ttraining's multi_logloss: 0.867179\tvalid_1's multi_logloss: 1.09235\n",
      "[1700]\ttraining's multi_logloss: 0.855594\tvalid_1's multi_logloss: 1.0921\n",
      "[1800]\ttraining's multi_logloss: 0.844381\tvalid_1's multi_logloss: 1.09171\n",
      "[1900]\ttraining's multi_logloss: 0.833403\tvalid_1's multi_logloss: 1.09171\n",
      "[2000]\ttraining's multi_logloss: 0.822695\tvalid_1's multi_logloss: 1.09166\n",
      "[2100]\ttraining's multi_logloss: 0.812266\tvalid_1's multi_logloss: 1.0916\n",
      "[2200]\ttraining's multi_logloss: 0.801877\tvalid_1's multi_logloss: 1.09158\n",
      "[2300]\ttraining's multi_logloss: 0.791716\tvalid_1's multi_logloss: 1.0918\n",
      "[2400]\ttraining's multi_logloss: 0.781973\tvalid_1's multi_logloss: 1.09207\n",
      "Early stopping, best iteration is:\n",
      "[2177]\ttraining's multi_logloss: 0.804261\tvalid_1's multi_logloss: 1.09147\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.14446\tvalid_1's multi_logloss: 1.14232\n",
      "[200]\ttraining's multi_logloss: 1.10378\tvalid_1's multi_logloss: 1.12153\n",
      "[300]\ttraining's multi_logloss: 1.0745\tvalid_1's multi_logloss: 1.11186\n",
      "[400]\ttraining's multi_logloss: 1.05006\tvalid_1's multi_logloss: 1.10567\n",
      "[500]\ttraining's multi_logloss: 1.02884\tvalid_1's multi_logloss: 1.10199\n",
      "[600]\ttraining's multi_logloss: 1.01005\tvalid_1's multi_logloss: 1.1001\n",
      "[700]\ttraining's multi_logloss: 0.993267\tvalid_1's multi_logloss: 1.09897\n",
      "[800]\ttraining's multi_logloss: 0.977331\tvalid_1's multi_logloss: 1.09852\n",
      "[900]\ttraining's multi_logloss: 0.962062\tvalid_1's multi_logloss: 1.09775\n",
      "[1000]\ttraining's multi_logloss: 0.947606\tvalid_1's multi_logloss: 1.09746\n",
      "[1100]\ttraining's multi_logloss: 0.933848\tvalid_1's multi_logloss: 1.09746\n",
      "[1200]\ttraining's multi_logloss: 0.920507\tvalid_1's multi_logloss: 1.09756\n",
      "[1300]\ttraining's multi_logloss: 0.907507\tvalid_1's multi_logloss: 1.09748\n",
      "Early stopping, best iteration is:\n",
      "[1041]\ttraining's multi_logloss: 0.941872\tvalid_1's multi_logloss: 1.09733\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's multi_logloss: 1.13998\tvalid_1's multi_logloss: 1.16689\n",
      "[200]\ttraining's multi_logloss: 1.09852\tvalid_1's multi_logloss: 1.14604\n",
      "[300]\ttraining's multi_logloss: 1.0688\tvalid_1's multi_logloss: 1.13558\n",
      "[400]\ttraining's multi_logloss: 1.04421\tvalid_1's multi_logloss: 1.13042\n",
      "[500]\ttraining's multi_logloss: 1.02277\tvalid_1's multi_logloss: 1.1275\n",
      "[600]\ttraining's multi_logloss: 1.0032\tvalid_1's multi_logloss: 1.12549\n",
      "[700]\ttraining's multi_logloss: 0.985303\tvalid_1's multi_logloss: 1.12377\n",
      "[800]\ttraining's multi_logloss: 0.968704\tvalid_1's multi_logloss: 1.12264\n",
      "[900]\ttraining's multi_logloss: 0.953065\tvalid_1's multi_logloss: 1.12191\n",
      "[1000]\ttraining's multi_logloss: 0.937979\tvalid_1's multi_logloss: 1.12166\n",
      "[1100]\ttraining's multi_logloss: 0.923691\tvalid_1's multi_logloss: 1.12167\n",
      "[1200]\ttraining's multi_logloss: 0.909903\tvalid_1's multi_logloss: 1.12156\n",
      "[1300]\ttraining's multi_logloss: 0.896503\tvalid_1's multi_logloss: 1.12133\n",
      "[1400]\ttraining's multi_logloss: 0.883616\tvalid_1's multi_logloss: 1.12133\n",
      "[1500]\ttraining's multi_logloss: 0.871089\tvalid_1's multi_logloss: 1.12111\n",
      "[1600]\ttraining's multi_logloss: 0.858825\tvalid_1's multi_logloss: 1.12114\n",
      "[1700]\ttraining's multi_logloss: 0.846991\tvalid_1's multi_logloss: 1.1213\n",
      "[1800]\ttraining's multi_logloss: 0.835471\tvalid_1's multi_logloss: 1.12139\n",
      "Early stopping, best iteration is:\n",
      "[1578]\ttraining's multi_logloss: 0.861458\tvalid_1's multi_logloss: 1.12107\n"
     ]
    }
   ],
   "source": [
    "# def fit_fold(df, train_ix, test_ix):\n",
    "#     train = df.iloc[train_ix].reset_index().copy()\n",
    "#     test = df.iloc[test_ix].reset_index().copy()\n",
    "#     x_train_all, x_val_all, x_test_all, y_train, y_val = make_features(train, test)\n",
    "    \n",
    "#     baseline = train_baseline(x_train_all, y_train, x_val_all, y_val)\n",
    "#     test_pred = baseline.predict(x_test_all).argmax(axis=1)\n",
    "#     test_true = test.accuracy_group.values\n",
    "#     return test_true, test_pred\n",
    "\n",
    "\n",
    "# def cross_validate(train, labels):\n",
    "#     predicts = []\n",
    "#     for ix_train, ix_test in cv.split(train, labels, train.installation_id.values):\n",
    "#         predicts.append(fit_fold(train, ix_train, ix_test))\n",
    "#     return predicts\n",
    "\n",
    "\n",
    "def make_features_wrapper(train, test):\n",
    "    x_train_all, x_val_all, x_test_all, y_train, y_val = make_features(train, test)\n",
    "    return (x_train_all,x_val_all,y_train,y_val), (x_test_all,test.accuracy_group.values)\n",
    "\n",
    "def make_predictions(model,x_test_all,y_test):\n",
    "    pred=model.predict(x_test_all).argmax(axis=1)\n",
    "    return pred,y_test\n",
    "\n",
    "predicts=cross_validate(train, train.accuracy_group.values, make_features_wrapper,train_baseline,make_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27631502524174467,\n",
       " [0.29748064769725047,\n",
       "  0.2628900684015911,\n",
       "  0.21348999589221673,\n",
       "  0.37431954892073294,\n",
       "  0.29026936480202903,\n",
       "  0.22334322193984346,\n",
       "  0.2688461802566625,\n",
       "  0.3103661998964561,\n",
       "  0.22402577966068105,\n",
       "  0.2981192449499831])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([quad_kappa(true, pred) for pred, true in predicts]), [quad_kappa(true, pred) for pred, true in predicts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
